{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/GSUUzLmPctx/vE+aQYwC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPoGGF4CpYiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d93d7b-d3d7-403c-f8ec-3c80bb26d5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.28.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (41.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.5.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Collecting typing-extensions>=4.3.0 (from azure-storage-blob)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.11.17)\n",
            "Installing collected packages: typing-extensions, isodate, azure-core, azure-storage-blob\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed azure-core-1.29.5 azure-storage-blob-12.19.0 isodate-0.6.1 typing-extensions-4.8.0\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=559ffa536a6ca23e5e07528a82ac03a0e796f2faec8169d8d5f10fee2513199c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "!{sys.executable} -m pip install azure-storage-blob\n",
        "!{sys.executable} -m pip install pyarrow\n",
        "!{sys.executable} -m pip install pandas\n",
        "!{sys.executable} -m pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "\n",
        "class YellowTaxi():\n",
        "    def __init__(self, start_date, end_date):\n",
        "        self.azure_storage_account_name = \"azureopendatastorage\"\n",
        "        self.azure_storage_sas_token = r\"\"\n",
        "        self.container_name = \"nyctlc\"\n",
        "        self.folder_name = \"yellow\"\n",
        "        self.start_date=start_date\n",
        "        self.end_date=end_date\n",
        "\n",
        "    def months_and_dates_between(self,start_date, end_date):\n",
        "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "        current_date = start\n",
        "        all_dates = []\n",
        "        while current_date <= end:\n",
        "            all_dates.append(current_date.strftime('%Y-%-m'))  # Formatting for YYYY-M\n",
        "            current_date += timedelta(days=1)\n",
        "        unique_months = list(set(all_dates))\n",
        "        return unique_months\n",
        "\n",
        "    def access_data(self):\n",
        "        print('Looking for the first parquet under the folder ' + self.folder_name + ' in container \"' + self.container_name + '\"...')\n",
        "        container_url = f\"https://{self.azure_storage_account_name}.blob.core.windows.net/\"\n",
        "        blob_service_client = BlobServiceClient(\n",
        "                                    container_url,\n",
        "                                    self.azure_storage_sas_token if self.azure_storage_sas_token else None\n",
        "                                )\n",
        "        container_client = blob_service_client.get_container_client(self.container_name)\n",
        "        blobs = container_client.list_blobs(self.folder_name)\n",
        "        return blobs, container_client\n",
        "\n",
        "    def get_data(self):\n",
        "        blobs, container_client=self.access_data()\n",
        "        dates_to_check=self.months_and_dates_between(self.start_date,self.end_date)\n",
        "        targetBlobName = []\n",
        "        for blob in blobs:\n",
        "            if blob.name.startswith(self.folder_name) and blob.name.endswith('.parquet'):\n",
        "                partition_date=str(blob).split('/')[1].replace('puYear=','')+'-'+str(blob).split('/')[2].replace('puMonth=','')\n",
        "                if partition_date in dates_to_check:\n",
        "                    targetBlobName.append(blob.name)\n",
        "\n",
        "        for file_name in targetBlobName:\n",
        "            _, filename = os.path.split(str(file_name))\n",
        "            blob_client = container_client.get_blob_client(file_name)\n",
        "            with open(filename, 'wb') as local_file:\n",
        "                try:\n",
        "                    blob_client.download_blob().download_to_stream(local_file)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtSnvY8jp0jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from itertools import chain\n",
        "import logging\n",
        "import os\n",
        "\n",
        "\n",
        "class Summarize():\n",
        "    def __init__(self,\n",
        "\n",
        "                start_date=None,\n",
        "                end_date=None):\n",
        "        self.data=None\n",
        "        self.master_list=[]\n",
        "        self.start_date=start_date\n",
        "        self.end_date=end_date\n",
        "        self.pwd=os.getcwd()\n",
        "        self.spark = SparkSession \\\n",
        "                    .builder \\\n",
        "                    .appName(\"Gray Matter Analytics\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        self.master_list = []\n",
        "        for filename in os.listdir(self.pwd):\n",
        "            _,extension=os.path.splitext(filename)\n",
        "            if extension=='.parquet':\n",
        "               self.master_list.append(filename)\n",
        "        print('here')\n",
        "        self.data=self.spark.read.parquet(*self.master_list)\n",
        "\n",
        "    def cleanUp(self):\n",
        "        for filename in self.master_list:\n",
        "            os.remove(f\"{self.pwd}/{filename}\")\n",
        "\n",
        "    def transformation(self):\n",
        "        columns=['paymentType',\n",
        "                 'totalAmount',\n",
        "                 'tipAmount',\n",
        "                 'tollsAmount',\n",
        "                 'fareAmount',\n",
        "                 'extra',\n",
        "                 'mtaTax',\n",
        "                 'improvementSurcharge',\n",
        "                 'tpepPickupDateTime',\n",
        "                 'tpepDropoffDateTime',\n",
        "                 'passengerCount',\n",
        "                 'tripDistance',\n",
        "                 'rateCodeId',\n",
        "                 'vendorID'\n",
        "                 ]\n",
        "        subset=self.data.select(*columns)\n",
        "        subset=subset.filter(\n",
        "            (to_date(col('tpepPickupDateTime'))>=self.start_date) & (to_date(col('tpepPickupDateTime'))<=self.end_date)\n",
        "        )\n",
        "        conditions = {\n",
        "            \"paymentType\" : {\n",
        "                \"1\":\"Credit card\",\n",
        "                \"2\": \"Cash\",\n",
        "                \"3\": \"No charge\",\n",
        "                \"4\": \"Dispute\" ,\n",
        "                \"5\": \"Unknown\",\n",
        "                \"6\": \"Voided trip\"\n",
        "            },\n",
        "            \"rateCodeId\" : {\n",
        "                \"1\": \"Standard rate\",\n",
        "                \"2\": \"JFK\",\n",
        "                \"3\": \"Newark\",\n",
        "                \"4\": \"Nassau or Westchester\",\n",
        "                \"5\": \"Negotiated fare\",\n",
        "                \"6\": \"Group ride\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for col_name in conditions.keys():\n",
        "            mapping_expr = create_map([lit(x) for x in chain(*conditions[col_name].items())])\n",
        "            subset=subset.withColumn(f\"{col_name}Code\", mapping_expr[col(col_name)])\n",
        "        subset=subset.withColumn (\"TripDurationMinutes\",round((unix_timestamp(col(\"tpepDropoffDateTime\")) - unix_timestamp(col(\"tpepPickupDateTime\"))) / 60))\n",
        "        subset=subset.withColumn('year',year(col('tpepPickupDateTime'))).withColumn('month',month(col('tpepPickupDateTime')))\n",
        "        subset=subset.drop('paymentType','rateCodeId')\n",
        "        return subset\n",
        "\n",
        "    def primary_analysis(self,df):\n",
        "        print(\"Generating primary analysis...\")\n",
        "        df=df.groupBy(\n",
        "            'paymentTypeCode',\n",
        "            'year',\n",
        "            'month'\n",
        "        ).agg(\n",
        "            round(mean('fareAmount'),2).alias('meanFareAmount'),\n",
        "            round(mean('totalAmount'),2).alias('meanTotalAmount'),\n",
        "            round(mean('passengerCount')).alias('meanPassengerCount'),\n",
        "            round(median('fareAmount'),2).alias('medianFareAmount'),\n",
        "            round(median('totalAmount'),2).alias('medianTotalAmount'),\n",
        "            round(median('passengerCount')).alias('medianPassengerCount'),\n",
        "            sum('passengerCount').alias('TotalPassengers')\n",
        "        ).orderBy(\n",
        "            'month',\n",
        "            'year'\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def secondary_analysis(self, df):\n",
        "        print(\"Generating secondary analysis....\")\n",
        "        df=df.groupBy(\n",
        "            'paymentTypeCode',\n",
        "            'rateCodeIdCode',\n",
        "            'year',\n",
        "            'month'\n",
        "        ).agg(\n",
        "            count('paymentTypeCode').alias('TotalTrips'),\n",
        "            round(sum('totalAmount'),2).alias('totalRevenueByPayment'),\n",
        "            round(sum('tipAmount'),2).alias('totalTipsCollected'),\n",
        "            round(sum('tollsAmount'),2).alias('totalTollsPaid')\n",
        "        ).orderBy(\n",
        "            'month',\n",
        "            'year'\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def generate_report(self, tabs=[]):\n",
        "        for sheet in tabs:\n",
        "            sheet[0].write.mode('overwrite').csv(f\"{self.pwd}/{sheet[1]}\",header=True)\n",
        "\n",
        "    def analysis(self):\n",
        "        data=self.transformation()\n",
        "        distinct_payments=data.select('paymentTypeCode').distinct()\n",
        "        print(f\"The number of distinct payment types are : {distinct_payments.count()} \\n\")\n",
        "\n",
        "        average_trip_duration=data.select(avg('TripDurationMinutes')).collect()[0][0]\n",
        "        print(f\"Average Trip Duration in Minutes: {average_trip_duration} \\n\")\n",
        "\n",
        "        print(f\"Total trips including voided and non-chargeable transactions: {data.count()} \\n\")\n",
        "\n",
        "        invalid_trips=data.filter(col('paymentTypeCode').isin('Dispute','Voided trip'))\n",
        "        print(f\"Total disputed or invalid-trips: {invalid_trips.count()} \\n\")\n",
        "\n",
        "        valid_trips=data.filter(col('paymentTypeCode').isin('Cash','Credit card'))\n",
        "        print(f\"Total trips paid by Cash/Credit {valid_trips.count()} \\n\")\n",
        "\n",
        "        total_analyzed=self.primary_analysis(data)\n",
        "        valid_analyzed=self.primary_analysis(valid_trips)\n",
        "        invalid_analyzed=self.primary_analysis(invalid_trips)\n",
        "\n",
        "        self.generate_report([\n",
        "            [total_analyzed,'total_analyzed/total_primary.csv'],\n",
        "            [valid_analyzed,'valid_analyzed/valid_primary.csv'],\n",
        "            [invalid_analyzed,'invalid_analyzed/invalid_primary.csv']\n",
        "        ])\n",
        "\n",
        "        total_analyzed=self.secondary_analysis(data)\n",
        "        valid_analyzed=self.secondary_analysis(valid_trips)\n",
        "        invalid_analyzed=self.secondary_analysis(invalid_trips)\n",
        "\n",
        "        self.generate_report([\n",
        "            [total_analyzed,'total_analyzed/total_secondary.csv'],\n",
        "            [valid_analyzed,'valid_analyzed/valid_secondary.csv'],\n",
        "            [invalid_analyzed,'invalid_analyzed/invalid_secondary.csv']\n",
        "        ])\n",
        "\n"
      ],
      "metadata": {
        "id": "ygIKFWUYqv24"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_object=YellowTaxi(start_date='2018-05-1',end_date='2018-07-1')\n",
        "data_object.get_data()\n",
        "summary=Summarize(start_date='2018-05-1',end_date='2018-07-1')\n",
        "summary.create_dataset()\n",
        "summary.analysis()\n",
        "summary.cleanUp()"
      ],
      "metadata": {
        "id": "kZ8fki3-IBKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4095670-cd3d-42bf-b211-448083740a71"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for the first parquet under the folder yellow in container \"nyctlc\"...\n",
            "here\n",
            "The number of distinct payment types are : 4 \n",
            "\n",
            "Average Trip Duration in Minutes: 17.641518487182626 \n",
            "\n",
            "Total trips including voided and non-chargeable transactions: 18173456 \n",
            "\n",
            "Total disputed or invalid-trips: 27615 \n",
            "\n",
            "Total trips paid by Cash/Credit 18046919 \n",
            "\n",
            "Generating primary analysis...\n",
            "Generating primary analysis...\n",
            "Generating primary analysis...\n",
            "Generating secondary analysis....\n",
            "Generating secondary analysis....\n",
            "Generating secondary analysis....\n",
            "/content/part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-118.c000.snappy.parquet\n",
            "/content/part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-118.c000.snappy.parquet\n",
            "/content/part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-121.c000.snappy.parquet\n",
            "/content/part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-117.c000.snappy.parquet\n",
            "/content/part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-119.c000.snappy.parquet\n",
            "/content/part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-118.c000.snappy.parquet\n",
            "/content/part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-118.c000.snappy.parquet\n",
            "/content/part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-119.c000.snappy.parquet\n",
            "/content/part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-116.c000.snappy.parquet\n",
            "/content/part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-120.c000.snappy.parquet\n",
            "/content/part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-117.c000.snappy.parquet\n",
            "/content/part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-118.c000.snappy.parquet\n",
            "/content/part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-118.c000.snappy.parquet\n",
            "/content/part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-115.c000.snappy.parquet\n",
            "/content/part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-119.c000.snappy.parquet\n",
            "/content/part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-118.c000.snappy.parquet\n",
            "/content/part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-119.c000.snappy.parquet\n",
            "/content/part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-118.c000.snappy.parquet\n",
            "/content/part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-118.c000.snappy.parquet\n",
            "/content/part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-118.c000.snappy.parquet\n",
            "/content/part-00002-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426334-119.c000.snappy.parquet\n",
            "/content/part-00004-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426331-116.c000.snappy.parquet\n",
            "/content/part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-117.c000.snappy.parquet\n",
            "/content/part-00017-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426323-120.c000.snappy.parquet\n",
            "/content/part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-120.c000.snappy.parquet\n",
            "/content/part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-119.c000.snappy.parquet\n",
            "/content/part-00015-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426342-117.c000.snappy.parquet\n",
            "/content/part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-117.c000.snappy.parquet\n",
            "/content/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-118.c000.snappy.parquet\n",
            "/content/part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-120.c000.snappy.parquet\n",
            "/content/part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-117.c000.snappy.parquet\n",
            "/content/part-00006-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426326-116.c000.snappy.parquet\n",
            "/content/part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-119.c000.snappy.parquet\n",
            "/content/part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-116.c000.snappy.parquet\n",
            "/content/part-00019-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426333-116.c000.snappy.parquet\n",
            "/content/part-00014-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426330-119.c000.snappy.parquet\n",
            "/content/part-00003-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426340-117.c000.snappy.parquet\n",
            "/content/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-120.c000.snappy.parquet\n",
            "/content/part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-117.c000.snappy.parquet\n",
            "/content/part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-118.c000.snappy.parquet\n",
            "/content/part-00009-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426325-117.c000.snappy.parquet\n",
            "/content/part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-120.c000.snappy.parquet\n",
            "/content/part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-117.c000.snappy.parquet\n",
            "/content/part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-117.c000.snappy.parquet\n",
            "/content/part-00010-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426335-118.c000.snappy.parquet\n",
            "/content/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-119.c000.snappy.parquet\n",
            "/content/part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-120.c000.snappy.parquet\n",
            "/content/part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-119.c000.snappy.parquet\n",
            "/content/part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-118.c000.snappy.parquet\n",
            "/content/part-00005-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426324-118.c000.snappy.parquet\n",
            "/content/part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-119.c000.snappy.parquet\n",
            "/content/part-00013-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426327-119.c000.snappy.parquet\n",
            "/content/part-00018-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426329-119.c000.snappy.parquet\n",
            "/content/part-00008-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426341-118.c000.snappy.parquet\n",
            "/content/part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-116.c000.snappy.parquet\n",
            "/content/part-00012-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426337-117.c000.snappy.parquet\n",
            "/content/part-00011-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426338-118.c000.snappy.parquet\n",
            "/content/part-00007-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426332-118.c000.snappy.parquet\n",
            "/content/part-00016-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426328-117.c000.snappy.parquet\n",
            "/content/part-00001-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426336-119.c000.snappy.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "hjqVDuZz9h7g",
        "outputId": "d97b3474-0bdd-44df-f724-41211a4882fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-8acf7ab9e702>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_analyzed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'total_analyzed.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jslp7mv2OJro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}