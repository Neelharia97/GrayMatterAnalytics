{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMxSQ2gaNVaJXCCKN2MJco"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPoGGF4CpYiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d93d7b-d3d7-403c-f8ec-3c80bb26d5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-storage-blob\n",
            "  Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.28.0 (from azure-storage-blob)\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (41.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.5.0)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Collecting typing-extensions>=4.3.0 (from azure-storage-blob)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.11.17)\n",
            "Installing collected packages: typing-extensions, isodate, azure-core, azure-storage-blob\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed azure-core-1.29.5 azure-storage-blob-12.19.0 isodate-0.6.1 typing-extensions-4.8.0\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=559ffa536a6ca23e5e07528a82ac03a0e796f2faec8169d8d5f10fee2513199c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "!{sys.executable} -m pip install azure-storage-blob\n",
        "!{sys.executable} -m pip install pyarrow\n",
        "!{sys.executable} -m pip install pandas\n",
        "!{sys.executable} -m pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Data-Prep Block\n",
        "\n",
        "The code-block below is responsible for eggressing parquet files from azure-container.\n",
        "The YellowTaxi class is initialized with start and end dates as inputs.\n",
        "\n",
        "`start_date`: Date for first taxi pick-up/transaction\n",
        "\n",
        "`end_date`: Date for last taxi pick-up/transaction\n",
        "\n",
        "---\n",
        "\n",
        "## Usage\n",
        "\n",
        "```\n",
        "data_object=YelloTaxi(start_date='2018-05-1',end_date='2018-07-1')\n",
        "data_object.get_data()\n",
        "\n",
        "```\n",
        "data_object is an instance to the class YellowTaxi with pulls data from NYCTaxiLimoYellow data from Azure Blob-Container.\n",
        "\n",
        "The instance will download parquet files according to start and end dates to the current working directory.\n",
        "\n",
        "---\n",
        "\n",
        "## Functions within the class\n",
        "\n",
        "`months_and_dates_between`: Generates a list of all unique months between start and end date.\n",
        "\n",
        "`access_data`: Returns blobs and connection client to access data from Azure\n",
        "\n",
        "`get_data`: Downloads the data from Azure as multiple parquet files in the current working directory\n"
      ],
      "metadata": {
        "id": "mCN7c4ril-em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "\n",
        "class YellowTaxi():\n",
        "    def __init__(self, start_date, end_date):\n",
        "        self.azure_storage_account_name = \"azureopendatastorage\"\n",
        "        self.azure_storage_sas_token = r\"\"\n",
        "        self.container_name = \"nyctlc\"\n",
        "        self.folder_name = \"yellow\"\n",
        "        self.start_date=start_date\n",
        "        self.end_date=end_date\n",
        "\n",
        "    def months_and_dates_between(self,start_date, end_date):\n",
        "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "        current_date = start\n",
        "        all_dates = []\n",
        "        while current_date <= end:\n",
        "            all_dates.append(current_date.strftime('%Y-%-m'))  # Formatting for YYYY-M\n",
        "            current_date += timedelta(days=1)\n",
        "        unique_months = list(set(all_dates))\n",
        "        return unique_months\n",
        "\n",
        "    def access_data(self):\n",
        "        print('Looking for the first parquet under the folder ' + self.folder_name + ' in container \"' + self.container_name + '\"...')\n",
        "        container_url = f\"https://{self.azure_storage_account_name}.blob.core.windows.net/\"\n",
        "        blob_service_client = BlobServiceClient(\n",
        "                                    container_url,\n",
        "                                    self.azure_storage_sas_token if self.azure_storage_sas_token else None\n",
        "                                )\n",
        "        container_client = blob_service_client.get_container_client(self.container_name)\n",
        "        blobs = container_client.list_blobs(self.folder_name)\n",
        "        return blobs, container_client\n",
        "\n",
        "    def get_data(self):\n",
        "        blobs, container_client=self.access_data()\n",
        "        dates_to_check=self.months_and_dates_between(self.start_date,self.end_date)\n",
        "        targetBlobName = []\n",
        "        for blob in blobs:\n",
        "            if blob.name.startswith(self.folder_name) and blob.name.endswith('.parquet'):\n",
        "                partition_date=str(blob).split('/')[1].replace('puYear=','')+'-'+str(blob).split('/')[2].replace('puMonth=','')\n",
        "                if partition_date in dates_to_check:\n",
        "                    targetBlobName.append(blob.name)\n",
        "\n",
        "        for file_name in targetBlobName:\n",
        "            _, filename = os.path.split(str(file_name))\n",
        "            blob_client = container_client.get_blob_client(file_name)\n",
        "            with open(filename, 'wb') as local_file:\n",
        "                try:\n",
        "                    blob_client.download_blob().download_to_stream(local_file)\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtSnvY8jp0jP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze/Summarize Block\n",
        "\n",
        "This code block is responsible for generating primary and secondary analysis.\n",
        "\n",
        "`Primary Analysis`:\n",
        "\n",
        "        *   Mean/medium cost, prices and passenger counts.\n",
        "        *   Aggregated by payment type, year, month.\n",
        "\n",
        "`Secondary Analysis`:\n",
        "\n",
        "        *  Total trips, tolls, tips paid\n",
        "        *  Aggregated by Payment Type, rateCodeIdCode, year, month\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jMpiP-N5v8fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from itertools import chain\n",
        "import logging\n",
        "import os\n",
        "\n",
        "class Summarize():\n",
        "    def __init__(self,\n",
        "\n",
        "                start_date=None,\n",
        "                end_date=None):\n",
        "        self.data=None\n",
        "        self.master_list=[]\n",
        "        self.start_date=start_date\n",
        "        self.end_date=end_date\n",
        "        self.pwd=os.getcwd()\n",
        "        self.spark = SparkSession \\\n",
        "                    .builder \\\n",
        "                    .appName(\"Gray Matter Analytics\") \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "\n",
        "    def create_dataset(self):\n",
        "        self.master_list = []\n",
        "        for filename in os.listdir(self.pwd):\n",
        "            _,extension=os.path.splitext(filename)\n",
        "            if extension=='.parquet':\n",
        "               self.master_list.append(filename)\n",
        "        self.data=self.spark.read.parquet(*self.master_list)\n",
        "\n",
        "    def cleanUp(self):\n",
        "        for filename in self.master_list:\n",
        "            os.remove(f\"{self.pwd}/{filename}\")\n",
        "\n",
        "    def transformation(self):\n",
        "        columns=['paymentType',\n",
        "                 'totalAmount',\n",
        "                 'tipAmount',\n",
        "                 'tollsAmount',\n",
        "                 'fareAmount',\n",
        "                 'extra',\n",
        "                 'mtaTax',\n",
        "                 'improvementSurcharge',\n",
        "                 'tpepPickupDateTime',\n",
        "                 'tpepDropoffDateTime',\n",
        "                 'passengerCount',\n",
        "                 'tripDistance',\n",
        "                 'rateCodeId',\n",
        "                 'vendorID'\n",
        "                 ]\n",
        "        subset=self.data.select(*columns)\n",
        "        subset=subset.filter(\n",
        "            (to_date(col('tpepPickupDateTime'))>=self.start_date) & (to_date(col('tpepPickupDateTime'))<=self.end_date)\n",
        "        )\n",
        "        conditions = {\n",
        "            \"paymentType\" : {\n",
        "                \"1\":\"Credit card\",\n",
        "                \"2\": \"Cash\",\n",
        "                \"3\": \"No charge\",\n",
        "                \"4\": \"Dispute\" ,\n",
        "                \"5\": \"Unknown\",\n",
        "                \"6\": \"Voided trip\"\n",
        "            },\n",
        "            \"rateCodeId\" : {\n",
        "                \"1\": \"Standard rate\",\n",
        "                \"2\": \"JFK\",\n",
        "                \"3\": \"Newark\",\n",
        "                \"4\": \"Nassau or Westchester\",\n",
        "                \"5\": \"Negotiated fare\",\n",
        "                \"6\": \"Group ride\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for col_name in conditions.keys():\n",
        "            mapping_expr = create_map([lit(x) for x in chain(*conditions[col_name].items())])\n",
        "            subset=subset.withColumn(f\"{col_name}Code\", mapping_expr[col(col_name)])\n",
        "        subset=subset.withColumn (\"TripDurationMinutes\",round((unix_timestamp(col(\"tpepDropoffDateTime\")) - unix_timestamp(col(\"tpepPickupDateTime\"))) / 60))\n",
        "        subset=subset.withColumn('year',year(col('tpepPickupDateTime'))).withColumn('month',month(col('tpepPickupDateTime')))\n",
        "        subset=subset.drop('paymentType','rateCodeId')\n",
        "        return subset\n",
        "\n",
        "    def primary_analysis(self,df):\n",
        "        print(\"Generating primary analysis...\")\n",
        "        df=df.groupBy(\n",
        "            'paymentTypeCode',\n",
        "            'year',\n",
        "            'month'\n",
        "        ).agg(\n",
        "            round(mean('fareAmount'),2).alias('meanFareAmount'),\n",
        "            round(mean('totalAmount'),2).alias('meanTotalAmount'),\n",
        "            round(mean('passengerCount')).alias('meanPassengerCount'),\n",
        "            round(median('fareAmount'),2).alias('medianFareAmount'),\n",
        "            round(median('totalAmount'),2).alias('medianTotalAmount'),\n",
        "            round(median('passengerCount')).alias('medianPassengerCount'),\n",
        "            sum('passengerCount').alias('TotalPassengers')\n",
        "        ).orderBy(\n",
        "            'month',\n",
        "            'year'\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def secondary_analysis(self, df):\n",
        "        print(\"Generating secondary analysis....\")\n",
        "        df=df.groupBy(\n",
        "            'paymentTypeCode',\n",
        "            'rateCodeIdCode',\n",
        "            'year',\n",
        "            'month'\n",
        "        ).agg(\n",
        "            count('paymentTypeCode').alias('TotalTrips'),\n",
        "            round(sum('totalAmount'),2).alias('totalRevenueByPayment'),\n",
        "            round(sum('tipAmount'),2).alias('totalTipsCollected'),\n",
        "            round(sum('tollsAmount'),2).alias('totalTollsPaid')\n",
        "        ).orderBy(\n",
        "            'month',\n",
        "            'year'\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def generate_report(self, tabs=[]):\n",
        "        for sheet in tabs:\n",
        "            sheet[0].write.mode('overwrite').csv(f\"{self.pwd}/{sheet[1]}\",header=True)\n",
        "\n",
        "    def analysis(self):\n",
        "        data=self.transformation()\n",
        "        distinct_payments=data.select('paymentTypeCode').distinct()\n",
        "        print(f\"The number of distinct payment types are : {distinct_payments.count()} \\n\")\n",
        "\n",
        "        average_trip_duration=data.select(avg('TripDurationMinutes')).collect()[0][0]\n",
        "        print(f\"Average Trip Duration in Minutes: {average_trip_duration} \\n\")\n",
        "\n",
        "        print(f\"Total trips including voided and non-chargeable transactions: {data.count()} \\n\")\n",
        "\n",
        "        invalid_trips=data.filter(col('paymentTypeCode').isin('Dispute','Voided trip'))\n",
        "        print(f\"Total disputed or invalid-trips: {invalid_trips.count()} \\n\")\n",
        "\n",
        "        valid_trips=data.filter(col('paymentTypeCode').isin('Cash','Credit card'))\n",
        "        print(f\"Total trips paid by Cash/Credit {valid_trips.count()} \\n\")\n",
        "\n",
        "        total_analyzed=self.primary_analysis(data)\n",
        "        valid_analyzed=self.primary_analysis(valid_trips)\n",
        "        invalid_analyzed=self.primary_analysis(invalid_trips)\n",
        "\n",
        "        self.generate_report([\n",
        "            [total_analyzed,'total_analyzed/total_primary.csv'],\n",
        "            [valid_analyzed,'valid_analyzed/valid_primary.csv'],\n",
        "            [invalid_analyzed,'invalid_analyzed/invalid_primary.csv']\n",
        "        ])\n",
        "\n",
        "        total_analyzed=self.secondary_analysis(data)\n",
        "        valid_analyzed=self.secondary_analysis(valid_trips)\n",
        "        invalid_analyzed=self.secondary_analysis(invalid_trips)\n",
        "\n",
        "        self.generate_report([\n",
        "            [total_analyzed,'total_analyzed/total_secondary.csv'],\n",
        "            [valid_analyzed,'valid_analyzed/valid_secondary.csv'],\n",
        "            [invalid_analyzed,'invalid_analyzed/invalid_secondary.csv']\n",
        "        ])\n",
        "\n"
      ],
      "metadata": {
        "id": "ygIKFWUYqv24"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_object=YellowTaxi(start_date='2018-05-1',end_date='2018-07-1')\n",
        "data_object.get_data()\n",
        "summary=Summarize(start_date='2018-05-1',end_date='2018-07-1')\n",
        "summary.create_dataset()\n",
        "summary.analysis()\n",
        "summary.cleanUp()"
      ],
      "metadata": {
        "id": "kZ8fki3-IBKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69d381f-d95a-4a4e-a862-9cbfccdfe72e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for the first parquet under the folder yellow in container \"nyctlc\"...\n",
            "The number of distinct payment types are : 4 \n",
            "\n",
            "Average Trip Duration in Minutes: 17.641518487182626 \n",
            "\n",
            "Total trips including voided and non-chargeable transactions: 18173456 \n",
            "\n",
            "Total disputed or invalid-trips: 27615 \n",
            "\n",
            "Total trips paid by Cash/Credit 18046919 \n",
            "\n",
            "Generating primary analysis...\n",
            "Generating primary analysis...\n",
            "Generating primary analysis...\n",
            "Generating secondary analysis....\n",
            "Generating secondary analysis....\n",
            "Generating secondary analysis....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jslp7mv2OJro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}